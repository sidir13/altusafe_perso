import os
import csv
import re
import json
from collections import Counter
import spacy
from src.common.config import RESULTS_DIR, VOCAB_DATA_DIR

# -----------------------------
# Chemins
# -----------------------------
CSV_PATH = os.path.join(RESULTS_DIR, "transcriptions.csv")
MEDICAL_PATH = os.path.join(VOCAB_DATA_DIR, "medical_vocab.json")
OUTPUT_PATH = os.path.join(VOCAB_DATA_DIR, "optimized_vocab.json")

os.makedirs(VOCAB_DATA_DIR, exist_ok=True)

# -----------------------------
# Chargement du mod√®le fran√ßais
# -----------------------------
nlp = spacy.load("fr_core_news_sm")
stopwords = nlp.Defaults.stop_words

# Mots fran√ßais tr√®s fr√©quents √† exclure
COMMON_WORDS = {
    "oui", "non", "voila", "bien", "donc", "peut", "etre", "merci", "bonjour",
    "avez", "faire", "fait", "voil√†", "mettre", "dire", "aller", "voir", "ok",
    "tr√®s", "tout", "comme", "avec", "avoir", "√™tre", "question", "bon", "alors",
    "ben", "peux", "suis", "c'est", "d'accord", "hein", "euh"
}

def clean_word(word: str):
    """Nettoyage de base pour les tokens"""
    w = word.lower().strip()
    if len(w) < 3 or w in stopwords or w in COMMON_WORDS:
        return None
    if re.match(r"^[^a-zA-Z√Ä-√ø'-]+$", w):
        return None
    if any(c.isdigit() for c in w):
        return None
    return w

# -----------------------------
# √âtape 1 : Charger le vocabulaire m√©dical
# -----------------------------
with open(MEDICAL_PATH, "r", encoding="utf-8") as f:
    medical_vocab = set(json.load(f))

print(f"üß¨ Vocabulaire m√©dical : {len(medical_vocab)} mots charg√©s")

# -----------------------------
# √âtape 2 : Analyser le CSV pour trouver les mots fr√©quents
# -----------------------------
word_counter = Counter()

with open(CSV_PATH, "r", encoding="utf-8") as f:
    reader = csv.DictReader(f)
    for row in reader:
        text = row.get("transcription_text", "")
        words = re.findall(r"[a-zA-Z√Ä-√ø'-]+", text.lower())
        for w in words:
            cw = clean_word(w)
            if cw:
                word_counter[cw] += 1

# Top 200 mots les plus fr√©quents dans le corpus
top_common = {w for w, _ in word_counter.most_common(200)}

print(f"üìä Mots fr√©quents extraits : {len(top_common)}")

# -----------------------------
# √âtape 3 : Fusionner intelligemment
# -----------------------------
optimized_vocab = sorted(medical_vocab.union(top_common))

# -----------------------------
# √âtape 4 : Sauvegarde
# -----------------------------
with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
    json.dump(optimized_vocab, f, ensure_ascii=False, indent=2)

print(f"‚úÖ Vocabulaire optimis√© g√©n√©r√© : {len(optimized_vocab)} mots")
print(f"üíæ Sauvegard√© dans : {OUTPUT_PATH}")
print("üß† Exemple :", optimized_vocab[:30])
